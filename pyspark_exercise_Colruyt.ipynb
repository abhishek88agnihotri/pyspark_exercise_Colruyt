{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv35IEj2wRQBH/eYTxAjTc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek88agnihotri/pyspark_exercise_Colruyt/blob/main/pyspark_exercise_Colruyt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dKOR8JX1nJEt"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import environ\n",
        "import findspark"
      ],
      "metadata": {
        "id": "nBfBemz3lMv5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "0ISD3FF8mDG7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init spark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "w5PFJ-JzmMdz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# spark.sql.repl.eagerEval.enabled: Property used to format output tables better\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"cg-pyspark-assignment\")\n",
        "    .master(\"local\")\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "    .getOrCreate()\n",
        "  )\n",
        "\n",
        "spark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "CjOC2eximQJu",
        "outputId": "439d190f-61cd-4517-9f44-68b5ef26c99b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c360ef846d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cc45710f8415:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>cg-pyspark-assignment</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/clp-places > clp-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/okay-places > okay-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/spar-places > spar-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/dats-places > dats-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/cogo-colpnts > cogo-colpnts.json\n"
      ],
      "metadata": {
        "id": "iQk-W0yloEHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56ee3b5-bcac-41e5-982b-de8cb33b0808"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  223k    0  223k    0     0   179k      0 --:--:--  0:00:01 --:--:--  179k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  147k    0  147k    0     0   176k      0 --:--:-- --:--:-- --:--:--  175k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  165k    0  165k    0     0   178k      0 --:--:-- --:--:-- --:--:--  178k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 89162    0 89162    0     0   139k      0 --:--:-- --:--:-- --:--:--  139k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  240k    0  240k    0     0   276k      0 --:--:-- --:--:-- --:--:--  276k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import getLogger, Logger"
      ],
      "metadata": {
        "id": "FNO061kHoHlY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOGGER = getLogger()"
      ],
      "metadata": {
        "id": "AXsjTGDs3SRf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a logger object\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "file_handler = logging.FileHandler('assignment.log')\n",
        "file_handler.setFormatter(formatter)\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# Define the API endpoints\n",
        "api_endpoints = {\n",
        "    \"clp\": \"https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/clp-places\",\n",
        "    \"okay\": \"https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/okay-places\",\n",
        "    \"spar\": \"https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/spar-places\",\n",
        "    \"dats\": \"https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/dats-places\",\n",
        "    \"cogo\": \"https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/cogo-colpnts\"\n",
        "}\n",
        "\n",
        "# Function to download data from API and save as JSON files\n",
        "def download_data_from_api(api_endpoints, logger):\n",
        "    for brand, url in api_endpoints.items():\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            filename = f\"{brand}-places.json\"\n",
        "            with open(filename, 'wb') as file:\n",
        "                file.write(response.content)\n",
        "            logger.info(f\"Data downloaded for {brand} and saved as {filename}\")\n",
        "        else:\n",
        "            logger.error(f\"Failed to download data for {brand}. Status code: {response.status_code}\")\n",
        "\n",
        "# Example function provided\n",
        "def get_data_by_brand(brand, logger):\n",
        "    \"\"\"Fetch input data based on brand.\n",
        "\n",
        "    Please add a column to the data indicating the input brand\n",
        "    Please add minimum one sanity check for loading the data\n",
        "    Please log things you consider relevant\n",
        "\n",
        "    Args:\n",
        "        brand: allowed values are (clp, okay, spar, dats, cogo)\n",
        "        logger: Logger object for logging\n",
        "\n",
        "    Returns:\n",
        "        The relevant dataframe\n",
        "    \"\"\"\n",
        "    filename = f\"{brand}-places.json\"\n",
        "    try:\n",
        "        logger.info(f\"Processing data for brand: {brand}...\")\n",
        "        # Read JSON file and create DataFrame\n",
        "        df = spark.read.json(filename)\n",
        "        # Add brand column\n",
        "        df = df.withColumn(\"brand\", lit(brand))\n",
        "        logger.info(f\"Data processed for brand: {brand}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing data for brand: {brand}. Error: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Download data from API and save as JSON files\n",
        "download_data_from_api(api_endpoints, logger)"
      ],
      "metadata": {
        "id": "rAF78soC3c2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090da1ec-b43f-4c67-b43b-ab24fcdb6909"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Data downloaded for clp and saved as clp-places.json\n",
            "INFO:__main__:Data downloaded for okay and saved as okay-places.json\n",
            "INFO:__main__:Data downloaded for spar and saved as spar-places.json\n",
            "INFO:__main__:Data downloaded for dats and saved as dats-places.json\n",
            "INFO:__main__:Data downloaded for cogo and saved as cogo-places.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import ArrayType, StructType\n",
        "from pyspark.sql.functions import lit, col, when, split\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import functions as F\n",
        "import getpass\n",
        "\n",
        "# Function to adjust schema of DataFrame\n",
        "def adjust_schema(df, common_columns):\n",
        "    for col_name in common_columns:\n",
        "        if col_name not in df.columns:\n",
        "            df = df.withColumn(col_name, lit(None))\n",
        "    return df.select(list(common_columns))\n",
        "\n",
        "# Function to merge schemas of DataFrames\n",
        "def merge_schemas(schema1, schema2):\n",
        "    fields = {}\n",
        "    # Add fields from schema1\n",
        "    for field in schema1.fields:\n",
        "        fields[field.name] = field\n",
        "    # Add fields from schema2, handle conflicts\n",
        "    for field in schema2.fields:\n",
        "        if field.name not in fields:\n",
        "            fields[field.name] = field\n",
        "\n",
        "    #print(fields)\n",
        "    return StructType(list(fields.values()))\n",
        "\n",
        "# Fetch data from API endpoints for all brands and process into DataFrames\n",
        "dfs = {}\n",
        "for brand, url in api_endpoints.items():\n",
        "    df = get_data_by_brand(brand,logger)\n",
        "    dfs[brand] = df\n",
        "\n",
        "#print(dfs)\n",
        "\n",
        "# Merge schemas of DataFrames\n",
        "combined_schema = None\n",
        "for df in dfs.values():\n",
        "    if combined_schema is None:\n",
        "        combined_schema = df.schema\n",
        "    else:\n",
        "        combined_schema = merge_schemas(combined_schema, df.schema)\n",
        "\n",
        "print('combined schema of all the brands - \\n',combined_schema)\n",
        "\n",
        "# Extract common columns from all DataFrames\n",
        "common_columns = set(combined_schema.names)\n",
        "for df in dfs.values():\n",
        "    common_columns = common_columns.intersection(set(df.columns))\n",
        "\n",
        "print('extracting common columns of all the brands - \\n',common_columns)\n",
        "\n",
        "# Adjust schemas for each DataFrame\n",
        "for brand, df in dfs.items():\n",
        "    dfs[brand] = adjust_schema(df, common_columns)\n",
        "\n",
        "\n",
        "# Adjust data types of each DataFrame\n",
        "for brand, df in dfs.items():\n",
        "    # Drop the 'placeSearchOpeningHours' column if it exists\n",
        "    if 'placeSearchOpeningHours' in df.columns:\n",
        "        df = df.drop('placeSearchOpeningHours')\n",
        "    # Adjust data types of each column\n",
        "    for field in combined_schema.fields:\n",
        "        # Skip if the field is 'placeSearchOpeningHours'\n",
        "        if field.name == 'placeSearchOpeningHours':\n",
        "            continue\n",
        "        # Handle specific columns like 'temporaryClosures' separately\n",
        "        if field.name == 'temporaryClosures':\n",
        "            # Assuming the original data type of 'temporaryClosures' is an array of strings\n",
        "            if field.name in df.columns:\n",
        "                df = df.withColumn(field.name, F.array(F.struct(F.lit(None).alias('from'), F.lit(None).alias('till'))))\n",
        "        # For other columns, cast to their original data types\n",
        "        else:\n",
        "            if isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):\n",
        "                original_data_type = field.dataType\n",
        "                df = df.withColumn(field.name, col(field.name).cast(original_data_type))\n",
        "    dfs[brand] = df\n",
        "\n",
        "#print(dfs)\n",
        "\n",
        "# Perform the union operation\n",
        "combined_df = None\n",
        "for df in dfs.values():\n",
        "    if combined_df is None:\n",
        "        combined_df = df\n",
        "    else:\n",
        "        combined_df = combined_df.union(df)\n",
        "\n",
        "\n",
        "# Extract \"postal_code\" from \"address\"\n",
        "#combined_df.printSchema()\n",
        "combined_df = combined_df.withColumn(\"postal_code\", combined_df.address.postalcode)\n",
        "\n",
        "\n",
        "# Create new column \"province\" derived from \"postal_code\"\n",
        "combined_df = combined_df.withColumn(\"province\", when(combined_df.postal_code.startswith(\"1\"), \"Province 1\")\n",
        "                                     .when(combined_df.postal_code.startswith(\"2\"), \"Province 2\")\n",
        "                                     .otherwise(\"Unknown\"))\n",
        "\n",
        "\n",
        "# Transform geoCoordinates into lat and lon column\n",
        "combined_df = combined_df.withColumn(\"latitude\", combined_df[\"geoCoordinates\"][\"latitude\"]) \\\n",
        "                         .withColumn(\"longitude\", combined_df[\"geoCoordinates\"][\"longitude\"])\n",
        "\n",
        "combined_df.show(5)\n",
        "\n",
        "# One-hot-encode the handoverServices if the column exists\n",
        "if \"handoverServices\" in combined_df.columns:\n",
        "    indexer = StringIndexer(inputCol=\"handoverServices\", outputCol=\"handoverServicesIndex\")\n",
        "    encoder = OneHotEncoder(inputCol=\"handoverServicesIndex\", outputCol=\"handoverServicesVec\")\n",
        "    pipeline = Pipeline(stages=[indexer, encoder])\n",
        "    combined_df = pipeline.fit(combined_df).transform(combined_df)\n",
        "    #combined_df.show(5)\n",
        "else:\n",
        "    print(\"Column 'handoverServices' does not exist in the DataFrame.\")\n",
        "\n",
        "# Fetch the current username\n",
        "user_name = getpass.getuser()\n",
        "\n",
        "# Anonymize houseNumber and streetName columns for unauthorized users\n",
        "# Assuming anonymization involves replacing them with dummy values\n",
        "authorized_users = [\"authorized_user1\", \"authorized_user2\"]\n",
        "if user_name not in authorized_users:\n",
        "    combined_df = combined_df.withColumn(\"houseNumber\", lit(\"GDPR_SENSITIVE_VALUE\")) \\\n",
        "                             .withColumn(\"streetName\", lit(\"GDPR_SENSITIVE_VALUE\"))\n",
        "#combined_df.show(5)\n",
        "\n",
        "# Drop the 'temporaryClosures' column\n",
        "combined_df = combined_df.drop('temporaryClosures')\n",
        "\n",
        "\n",
        "# Save the end result as a Parquet file with partitioning\n",
        "combined_df.write.partitionBy(\"province\").parquet(\"brand_data.parquet\", mode=\"overwrite\")\n",
        "print(\"Data is saved in the file\")\n",
        "\n",
        "# Stop SparkSession\n",
        "# spark.stop()\n"
      ],
      "metadata": {
        "id": "0hoOKg154NFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07981fd9-6871-4e36-a1ad-82d4a1c5b6e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Processing data for brand: clp...\n",
            "INFO:__main__:Data processed for brand: clp\n",
            "INFO:__main__:Processing data for brand: okay...\n",
            "INFO:__main__:Data processed for brand: okay\n",
            "INFO:__main__:Processing data for brand: spar...\n",
            "INFO:__main__:Data processed for brand: spar\n",
            "INFO:__main__:Processing data for brand: dats...\n",
            "INFO:__main__:Data processed for brand: dats\n",
            "INFO:__main__:Processing data for brand: cogo...\n",
            "INFO:__main__:Data processed for brand: cogo\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "combined schema of all the brands - \n",
            " StructType([StructField('address', StructType([StructField('cityName', StringType(), True), StructField('countryCode', StringType(), True), StructField('countryName', StringType(), True), StructField('houseNumber', StringType(), True), StructField('postalcode', StringType(), True), StructField('streetName', StringType(), True)]), True), StructField('branchId', StringType(), True), StructField('commercialName', StringType(), True), StructField('ensign', StructType([StructField('id', LongType(), True), StructField('name', StringType(), True)]), True), StructField('geoCoordinates', StructType([StructField('latitude', DoubleType(), True), StructField('longitude', DoubleType(), True)]), True), StructField('handoverServices', ArrayType(StringType(), True), True), StructField('isActive', BooleanType(), True), StructField('moreInfoUrl', StringType(), True), StructField('placeId', LongType(), True), StructField('placeSearchOpeningHours', ArrayType(StructType([StructField('closes', LongType(), True), StructField('date', StringType(), True), StructField('isOpenForTheDay', BooleanType(), True), StructField('isToday', BooleanType(), True), StructField('opens', LongType(), True)]), True), True), StructField('placeType', StructType([StructField('id', LongType(), True), StructField('longName', StringType(), True), StructField('placeTypeDescription', StringType(), True)]), True), StructField('routeUrl', StringType(), True), StructField('sellingPartners', ArrayType(StringType(), True), True), StructField('sourceStatus', StringType(), True), StructField('temporaryClosures', ArrayType(StringType(), True), True), StructField('brand', StringType(), False)])\n",
            "extracting common columns of all the brands - \n",
            " {'ensign', 'sourceStatus', 'isActive', 'brand', 'geoCoordinates', 'placeType', 'moreInfoUrl', 'routeUrl', 'commercialName', 'placeId', 'branchId', 'address', 'temporaryClosures'}\n",
            "+-----------------+------------+--------+-----+--------------------+-------------------+--------------------+--------------------+-------------------+-------+--------+--------------------+-----------------+-----------+----------+----------+---------+\n",
            "|           ensign|sourceStatus|isActive|brand|      geoCoordinates|          placeType|         moreInfoUrl|            routeUrl|     commercialName|placeId|branchId|             address|temporaryClosures|postal_code|  province|  latitude|longitude|\n",
            "+-----------------+------------+--------+-----+--------------------+-------------------+--------------------+--------------------+-------------------+-------+--------+--------------------+-----------------+-----------+----------+----------+---------+\n",
            "|{8, COLR_Colruyt}|          AC|    true|  clp|{50.933074, 4.053...|{1, Winkel, Winkel}|https://www.colru...|https://maps.appl...|    AALST (COLRUYT)|    902|    4156|{AALST, BE, Belgi...|   [{null, null}]|       9300|   Unknown| 50.933074|4.0538972|\n",
            "|{8, COLR_Colruyt}|          AC|    true|  clp|{51.0784761, 3.45...|{1, Winkel, Winkel}|https://www.colru...|https://maps.appl...|   AALTER (COLRUYT)|    946|    4218|{AALTER, BE, Belg...|   [{null, null}]|       9880|   Unknown|51.0784761|3.4500133|\n",
            "|{8, COLR_Colruyt}|          AC|    true|  clp|{50.9760369, 4.81...|{1, Winkel, Winkel}|https://www.colru...|https://maps.appl...| AARSCHOT (COLRUYT)|    950|    4222|{AARSCHOT, BE, Be...|   [{null, null}]|       3200|   Unknown|50.9760369|4.8110969|\n",
            "|{8, COLR_Colruyt}|          AC|    true|  clp|{50.7415212, 4.33...|{1, Winkel, Winkel}|https://www.colru...|https://maps.appl...|ALSEMBERG (COLRUYT)|    886|    4138|{ALSEMBERG, BE, B...|   [{null, null}]|       1652|Province 1|50.7415212| 4.336719|\n",
            "|{8, COLR_Colruyt}|          AC|    true|  clp|{50.5599284, 5.30...|{1, Winkel, Winkel}|https://www.colru...|https://maps.appl...|     AMAY (COLRUYT)|    783|    3853|{AMAY, BE, België...|   [{null, null}]|       4540|   Unknown|50.5599284|5.3061951|\n",
            "+-----------------+------------+--------+-----+--------------------+-------------------+--------------------+--------------------+-------------------+-------+--------+--------------------+-----------------+-----------+----------+----------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Column 'handoverServices' does not exist in the DataFrame.\n",
            "Data is saved in the file\n"
          ]
        }
      ]
    }
  ]
}